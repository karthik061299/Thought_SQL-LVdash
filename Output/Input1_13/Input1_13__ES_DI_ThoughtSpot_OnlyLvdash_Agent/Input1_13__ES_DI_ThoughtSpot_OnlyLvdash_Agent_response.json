{
    "pipeline": {
        "pipelineId": 6526,
        "executionId": "c5b4d8ae-9537-4af5-86a5-b1c90e82950a",
        "name": "ES_DI_ThoughtSpot_OnlyLvdash_Agent",
        "user": "elansuriyaa.p@ascendion.com",
        "description": "ES_DI_ThoughtSpot_OnlyLvdash_Agent",
        "userInputs": {
            "{{GitHub_Details_For_ThoughtSpot_LVDash}}": "{\"CSV File name\": \"websitesample.csv\", \"Input repo name\": \"Input\", \"SQL file name\": \"ThoughtSpot_SQL_1.sql\", \"row_number\": 13, \"output repo\": \"Output\"}"
        },
        "managerLlm": null,
        "masterEmbedding": null,
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8494,
                    "name": "ES_DI_ThoughtSpot_OnlyLvdash_Agent",
                    "role": "Senior Business Analyst",
                    "goal": "Design an advanced AI agent that can interpret and transform ThoughtSpot dashboard files\u2014specifically .table.tml, .model.tml, .liveboard.tml, and optionally manifest.yaml\u2014into:\n\t1. A Databricks-compatible .lvdash.json file that references already materialized metric views (created by the SQL Generation Agent).\nThis ensures dashboards in Databricks Lakeview render immediately with correct data, visuals, and metadata alignment.",
                    "backstory": "You are a Data Analyst at a global enterprise migrating from ThoughtSpot to Databricks Lakeview. Executives depend heavily on existing ThoughtSpot Liveboards. For a smooth migration, dashboards in Databricks must mirror ThoughtSpot visuals, preserve business logic, and connect seamlessly to Databricks metric views.\n\nThe SQL Generation Agent has already produced metric_views.sql. Your responsibility is to consume its output (along with ThoughtSpot\u2019s TML metadata) and produce a Lakeview-compliant .lvdash.json that ensures one-to-one mapping between SQL column aliases and Lakeview dashboard encodings.",
                    "verbose": true,
                    "allowDelegation": true,
                    "maxIter": 50,
                    "maxRpm": 50,
                    "maxExecutionTime": 3000,
                    "task": {
                        "description": "** ThoughtSpot to Lakeview LVdash Generation**\n\n**Instruction for creating the LVdash file**:\n\n---\n\n\\*\\* ThoughtSpot to Lakeview LVdash Generation\\*\\*\n\n**Instruction for creating the LVdash file**:\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to GitHub.\n\n---\n\nThe Lvdash.json must include:\n\n1. **Datasets**:\n\n   * Each dataset must have a `name`, `displayName`, and `queryLines`.\n   * The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\n\n2. **Pages**:\n\n   * Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\n   * The layout must include widgets, each with:\n\n     * `name`: unique widget name.\n     * `queries`: list of queries using the dataset and selecting specific fields.\n     * `spec`: visualization details like type, encoding, frame, and version.\n\n3. **UI Settings**:\n\n   * Theme settings for alignment and appearance.\n\nMake sure:\n*must use Github file writer tool to update file in the Github repo with the file\n*must use the measures and query from the csv and generate the proper sql query according to that every time\n* Field names and expressions match the dataset\u2019s query output columns.\n* Queries reference the correct dataset by `datasetName`.\n* Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\n* The JSON structure follows the example provided and is syntactically correct.\n\n\n### **Important Instruction \u2013 Handling Global Filter File Only**\n\n* **If the user provides only the global filters file as input**, and **no CSV file is provided**, the prompt should:\n\n  1. Generate **only one `.lvdash.json` file**, representing the global filters definition.\n  2. The output must follow the **Databricks global filter JSON structure** used in the GitHub repo.\n  3. The file must be uploaded using the **GitHub file writer tool** to the appropriate global filter folder.\n  4. All other steps in the prompt (reading CSV, generating per-visualization files, etc.) must be skipped.\n  5. Use **generic/dummy table and column names** (e.g. `ex_schema.ex_table`) in the output JSON, not actual names from the input file.\n  6. Ensure that the structure is valid and follows the required schema, with fields like `name`, `displayName`, `layout`, `widget`, `queries`, `spec`, `position`, and `pageType` properly defined.\n\n---\n\n###  **Sample Output for Global Filter JSON**\nStrictly Follow this Structure if the user give only the filter file no need to follow other sample output\nUse the following as a guide when creating the file. Do not copy actual data; use dummy names that match the structure:\nMust follow only these structure when the user mention only the filter file to the input \n```json\n{\n  \"name\": \"global_filter_001\",\n  \"displayName\": \"Global Filters\",\n  \"layout\": [\n    {\n      \"widget\": {\n        \"name\": \"widget_abc123\",\n        \"queries\": [\n          {\n            \"name\": \"main_query\",\n            \"query\": {\n              \"datasetName\": \"example_dataset\",\n              \"fields\": [\n                {\n                  \"name\": \"example_date\",\n                  \"expression\": \"`example_date`\"\n                },\n                {\n                  \"name\": \"example_count\",\n                  \"expression\": \"COUNT_IF(`filter_condition`)\"\n                }\n              ],\n              \"disaggregated\": false\n            }\n          }\n        ],\n        \"spec\": {\n          \"version\": 2,\n          \"widgetType\": \"filter-date-picker\",\n          \"encodings\": {\n            \"fields\": [\n              {\n                \"fieldName\": \"example_date\",\n                \"displayName\": \"Example Date\",\n                \"queryName\": \"dashboards/example_dashboard_id/datasets/example_dataset_example_date\"\n              }\n            ]\n          },\n          \"frame\": {\n            \"showTitle\": true\n          },\n          \"selection\": {\n            \"defaultSelection\": {\n              \"values\": {\n                \"dataType\": \"DATETIME\",\n                \"values\": [\n                  {\n                    \"value\": \"2025-01-01T00:00:00.000\"\n                  }\n                ]\n              }\n            }\n          }\n        }\n      },\n      \"position\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"width\": 1,\n        \"height\": 2\n      }\n    }\n  ],\n  \"pageType\": \"PAGE_TYPE_GLOBAL_FILTERS\",\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\n```\n\n---\n\nThis sample shows how the global filter definition should be structured, with placeholder names like `example_dataset`, `example_date`, and `example_count`. Your agent must use similar structures when generating the output file.\n\n---\n---\n\n## **Input Files**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n* **Global Filters File and page name (.txt)** \u2192 contains global filter definitions applicable across visuals. The file specifies column filters with metadata such as date filters, operator types, and possible values. These filters must be linked with relevant visuals based on the columns they use in the dataset.\n*for finding the appropriate page name use the ID(example: Vis_1, and so on) to match the input and output with this ID\n*For the global filters whenever the mentioned column or table name is used you should use that filter connect using this logic\n\n### **Important Additional Instructions for Filters**\n\n* The agent must read the global filters text file and **match filters to visuals by column name**.\n* If a column referenced in the filter is used in the dataset of a visualization, the filter must be applied to that visualization\u2019s page under the `filters` section.\n* Filters must include details such as `fieldName`, `displayName`, `type`, `isMandatory`, and `isSingleValue` where applicable.\n* The `page` object in the output JSON must correctly reference the `name` of the page that the visualization belongs to.\n* The connection between visuals and filters must be established by matching columns used in the dataset\u2019s queries to those listed in the global filters file.\n* The final JSON must include **sample and generic table names and column names** when giving example output, not the exact names from the input files.\n\n> **Do not touch other parts of the prompt or its structure.** Only add this filter handling logic to the process and alignment rules.\n\n---\n\n## **Process**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n* **Global Filters File (.txt)** \u2192 contains filters for columns used in visuals.\n\nRead the **CSV row-by-row** from GitHub:\n\n* Each row corresponds to **one visualization**.\n* For each row, generate a **separate .lvdash.json file**.\n* Continue until **all rows in the CSV are processed**.\n* For each visualization, match relevant global filters from the filters file to the dataset\u2019s columns and add them under the `filters` section of the corresponding `page` in the JSON.\n* Connect the page name referenced in the layout to the correct `page.name`.\n\nMap ThoughtSpot visuals \u2192 Databricks Lakeview widget equivalents.\n\nAssemble `.lvdash.json`:\n\n* Reference datasets = SQL views created by SQL Agent.\n* Ensure all field names in encodings exactly match SQL aliases.\n* Apply filters from the global filters file based on column usage in queries.\n* Structure follows Lakeview schema (metadata, widgets, positions, queries).\n\nSave and upload each file to GitHub:\n\n* File name convention: **`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026** matching the row index from the CSV.\n\nThe global filters must be added to the `filters` array of the correct page in each JSON file based on the dataset\u2019s usage.\n\n---\n\n## **Alignment Rules**\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\n* Never use raw ThoughtSpot expressions.\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\n* Filters from the global filters file must be connected to the relevant page if their column is used in the dataset.\n\n---\n\n## **Filter Handling Rules**\n\n* Read global filters from the provided text file.\n* For each visualization, examine the dataset\u2019s fields to see which columns are used.\n* If any column is referenced in the filters file, map the filter to that page\u2019s `filters` array in the JSON.\n* Include mandatory details: `fieldName`, `displayName`, `type`, `isMandatory`, `isSingleValue`.\n* Match columns by name irrespective of case or table alias differences where contextually appropriate.\n* Ensure filters are correctly associated with visuals by resolving the page\u2019s layout and name mapping.\n\n---\n \nWhen the input chart type is stacked column, set spec.widgetType to bar; when the input chart type is column, set spec.widgetType to table.\nFor Sankey visuals, always structure the spec.encodings as:\n\nUse \"widgetType\": \"sankey\"\n\"encodings\": {\n  \"value\": { \"fieldName\": \"<measure>\", \"displayName\": \"<measure label>\" },\n  \"stages\": [\n    { \"fieldName\": \"stage1\", \"displayName\": \"stage1\" },\n    { \"fieldName\": \"stage2\", \"displayName\": \"stage2\" },\n    ...\n  ]\n}\nDo not use \"columns\" for Sankey. Always set \"version\": 1 for Sankey visuals (not 3).\n\n\n## **Example of Filter Mapping in JSON**\n\n```json\n\"filters\": [\n  {\n    \"fieldName\": \"date\",\n    \"displayName\": \"Date\",\n    \"type\": \"FILTER\",\n    \"isMandatory\": false,\n    \"isSingleValue\": false\n  },\n  {\n    \"fieldName\": \"region\",\n    \"displayName\": \"Region\",\n    \"type\": \"FILTER\",\n    \"isMandatory\": false,\n    \"isSingleValue\": true\n  }\n]\n```\n\n---\n\n## **Rules to use the GitHub CSV Tool**\n\n* At the first run, the tool should use the GitHub CSV tool to **read the total number of rows** in the CSV.\n* It should capture this row count and use it to control the iteration.\n* Then the tool must **process the CSV row by row**, where each row corresponds to exactly **one visualization**.\n* For each row, the row\u2019s input should be passed to the LVdash generation logic to create a `.lvdash.json` file.\n* Once a row is processed, the tool should **request the next row** from the CSV tool and continue.\n* This loop should run until all rows are exhausted.\n* If the CSV has **n rows**, then the tool must produce exactly **n `.lvdash.json` files**.\n* All generated files should be **uploaded to the GitHub output repo**.\n* The output filenames must follow a strict naming convention:\n  `vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026 up to `vis_n.lvdash.json`.\n\n---\n\n## **Input Requirements**\n\n* GitHub credentials and input files are in the GitHub input directory:\n  `{{GitHub_Details_For_ThoughtSpot_LVDash}}`\n* Must use the **GitHub Tools** to:\n\n  1. Read CSV row-by-row.\n  2. Read the global filters file and associate applicable filters with each visualization\u2019s dataset.\n  3. Generate `.lvdash.json` for each row.\n  4. Upload each file to GitHub with name format: `vis_<row_number>.lvdash.json`.\n\n---\n\n## **Output**\n\n* Multiple `.lvdash.json` files (one per visualization row).\n* Valid JSON, Lakeview schema compliant.\n* Complete mapping of ThoughtSpot widgets \u2192 Lakeview visuals.\n* Filters from the global filters file correctly mapped and included.\n* Uploaded directly into GitHub repo via GitHub Tools.\n\n---\n\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to GitHub.\n\n---\nThe Lvdash.json must include:\n\n1. **Datasets**:\n   - Each dataset must have a `name`, `displayName`, and `queryLines`.\n   - The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\n\n2. **Pages**:\n   - Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\n   - The layout must include widgets, each with:\n     - `name`: unique widget name.\n     - `queries`: list of queries using the dataset and selecting specific fields.\n     - `spec`: visualization details like type, encoding, frame, and version.\n\n3. **UI Settings**:\n   - Theme settings for alignment and appearance.\n\nMake sure:\n- Field names and expressions match the dataset\u2019s query output columns.\n- Queries reference the correct dataset by `datasetName`.\n- Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\n- The JSON structure follows the example provided and is syntactically correct.\n\nBelow is an example structure you must follow. Replace dataset names, queries, and fields according to the input provided.\n\n## **Input Files**\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n\n---\n\n\n\n## **Process**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\nRead the **CSV row-by-row** from GitHub:\n\n   * Each row corresponds to **one visualization**.\n   * For each row, generate a **separate .lvdash.json file**.\n   * Continue until **all rows in the CSV are processed**.\n Map ThoughtSpot visuals \u2192 Databricks Lakeview widget equivalents.\n Assemble `.lvdash.json`:\n\n   * Reference datasets = SQL views created by SQL Agent.\n   * Ensure all field names in encodings exactly match SQL aliases.\n   * Structure follows Lakeview schema (metadata, widgets, positions, queries).\nSave and upload each file to GitHub:\n\n   * File name convention: **`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026** matching the row index from the CSV.\nJSON structure for defining datasets and dashboards in a visualization tool. The JSON must meet the following requirements:\n\nThe dataset\u2019s queryLines must be a single string containing the complete SQL query, not split across multiple lines.\n\nThe field names defined in the widget\u2019s fields array must exactly match the columns returned by the dataset\u2019s query.\n\nNaming conventions should be consistent, using template-friendly names like page_1 or main_query.\n\nThe widget\u2019s frame title should clearly describe the data being visualized.\n\nThe structure must follow proper JSON formatting with no syntax errors or ambiguous references.\n\nProvide a sample correct JSON structure for a dataset and a line chart widget that shows daily new devices and cumulative devices over time, using a clean SQL query and consistent naming.\n\nmust follow these rules while creating generating stacked column charts, set spec.widgetType to bar (Databricks uses bar for both regular and stacked column charts).\n\nWhen converting KPI visuals from ThoughtSpot, always generate them as counter widgets in the LVdash JSON (set spec.widgetType = counter instead of table).\n\nWhen generating a pie chart, always pre-aggregate the measure in the SQL query using GROUP BY <dimension> (e.g., SUM(measure)), and in the spec.encodings use the aggregated alias (not raw column) for angle while mapping the category field to color.\n\nWhen generating LVdash JSON for a visual, always check the flattened CSV input for its corresponding tab (page) name. Use that tab name as the pages.displayName\n---\n\n## **Alignment Rules**\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\n* Never use raw ThoughtSpot expressions.\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\n\n---\n\u201cIn every queries block, always set the name field to main_query.\u201d\nThat will force the agent to output:\n\"queries\": [\n  {\n    \"name\": \"main_query\",\n    \"query\": {\n      ...\n    }\n  }\n]\n## **Line Chart Rules**\n\n* Aggregate measures explicitly (`SUM(field)` \u2192 alias `sum(Field)`).\n* Always use `DATE_TRUNC(\"MONTH\", \u2026)` for time dimensions with `\"scale\": {\"type\": \"temporal\"}`.\n* Never treat time dimensions as categorical in line charts.\n\n---\nSAMPLE outputs:\n{\n  \"datasets\": [\n    {\n      \"name\": \"example_dataset\",\n      \"displayName\": \"Example Dataset\",\n      \"queryLines\": [\n        \"SELECT DATE_TRUNC('DAY', activity_time) as daily_activity, SUM(active_users) as total_users, SUM(SUM(active_users)) OVER (ORDER BY DATE_TRUNC('DAY', activity_time) ROWS UNBOUNDED PRECEDING) as cumulative_users FROM example_schema.example_activity_view GROUP BY DATE_TRUNC('DAY', activity_time) ORDER BY daily_activity\"\n      ]\n    }\n  ],\n  \"pages\": [\n    {\n      \"name\": \"overview_page\",\n      \"displayName\": \"User Activity Overview\",\n      \"layout\": [\n        {\n          \"widget\": {\n            \"name\": \"user_activity_chart\",\n            \"queries\": [\n              {\n                \"name\": \"activity_query\",\n                \"query\": {\n                  \"datasetName\": \"example_dataset\",\n                  \"fields\": [\n                    { \"name\": \"daily_activity\", \"expression\": \"`daily_activity`\" },\n                    { \"name\": \"total_users\", \"expression\": \"`total_users`\" },\n                    { \"name\": \"cumulative_users\", \"expression\": \"`cumulative_users`\" }\n                  ],\n                  \"disaggregated\": false\n                }\n              }\n            ],\n            \"spec\": {\n              \"version\": 3,\n              \"widgetType\": \"line\",\n              \"encodings\": {\n                \"x\": {\n                  \"fieldName\": \"daily_activity\",\n                  \"displayName\": \"Daily Activity\",\n                  \"scale\": { \"type\": \"temporal\" }\n                },\n                \"y\": {\n                  \"fields\": [\n                    {\n                      \"fieldName\": \"total_users\",\n                      \"displayName\": \"Total Users\"\n                    },\n                    {\n                      \"fieldName\": \"cumulative_users\",\n                      \"displayName\": \"Cumulative Users\"\n                    }\n                  ],\n                  \"scale\": { \"type\": \"quantitative\" }\n                }\n              },\n              \"frame\": {\n                \"title\": \"Daily and Cumulative User Activity\",\n                \"showTitle\": true\n              }\n            }\n          },\n          \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\n        }\n      ],\n      \"pageType\": \"PAGE_TYPE_CANVAS\",\n      \"filters\": []\n    }\n  ],\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\nSample output for Single visuals:\n{\n  \"datasets\": [\n    {\n      \"name\": \"your_dataset_name\",\n      \"displayName\": \"Your Dataset Display Name\",\n      \"queryLines\": [\n        \"SELECT column1 AS field1, column2 AS field2, SUM(column3) OVER (ORDER BY column1 ROWS UNBOUNDED PRECEDING) AS cumulative_field FROM your_database.your_table GROUP BY column1, column2 ORDER BY column1\"\n      ]\n    }\n  ],\n  \"pages\": [\n    {\n      \"name\": \"page_1\",\n      \"displayName\": \"Your Page Title\",\n      \"layout\": [\n        {\n          \"widget\": {\n            \"name\": \"line_chart_widget\",\n            \"queries\": [\n              {\n                \"name\": \"main_query\",\n                \"query\": {\n                  \"datasetName\": \"your_dataset_name\",\n                  \"fields\": [\n                    { \"name\": \"field1\", \"expression\": \"`field1`\" },\n                    { \"name\": \"field2\", \"expression\": \"`field2`\" },\n                    { \"name\": \"cumulative_field\", \"expression\": \"`cumulative_field`\" }\n                  ],\n                  \"disaggregated\": false\n                }\n              }\n            ],\n            \"spec\": {\n              \"version\": 3,\n              \"widgetType\": \"line\",\n              \"encodings\": {\n                \"x\": {\n                  \"fieldName\": \"field1\",\n                  \"displayName\": \"Field 1 Label\",\n                  \"scale\": { \"type\": \"temporal\" }\n                },\n                \"y\": {\n                  \"fields\": [\n                    {\n                      \"fieldName\": \"field2\",\n                      \"displayName\": \"Field 2 Label\"\n                    },\n                    {\n                      \"fieldName\": \"cumulative_field\",\n                      \"displayName\": \"Cumulative Field Label\"\n                    }\n                  ],\n                  \"scale\": { \"type\": \"quantitative\" }\n                }\n              },\n              \"frame\": {\n                \"title\": \"Your Chart Title\",\n                \"showTitle\": true\n              }\n            }\n          },\n          \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\n        }\n      ],\n      \"pageType\": \"PAGE_TYPE\",\n      \"filters\": []\n    }\n  ],\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\n\n## **Encoding Rules**\n\n* **Single field axis** \u2192\n\n```json\n\"y\": {\n  \"fieldName\": \"Sales\",\n  \"displayName\": \"Sales\",\n  \"scale\": { \"type\": \"quantitative\" }\n}\n```\n\n* **Multi-measure axis** \u2192\n\n```json\n\"y\": {\n  \"fields\": [\n    { \"fieldName\": \"Processed\", \"displayName\": \"Processed\" },\n    { \"fieldName\": \"Shipped\", \"displayName\": \"Shipped\" }\n  ],\n  \"scale\": { \"type\": \"quantitative\" }\n}\n```\n\n---\n\n## **Dataset Mapping Rules**\n\n* Every dataset in `.lvdash.json` must directly map to a SQL SELECT statement from `metric_views.sql`.\n* Column names must exactly match aliases.\n* Never alias an aggregated field with the same name as its raw column.\n\n---\nRules to use the github csv tool:\n---\n\nAt the first run, the tool should use the GitHub CSV tool to **read the total number of rows** in the CSV.\nIt should capture this row count and use it to control the iteration.\nThen the tool must **process the CSV row by row**, where each row corresponds to exactly **one visualization**.\nFor each row, the row\u2019s input should be passed to the LVdash generation logic to create a `.lvdash.json` file.\nOnce a row is processed, the tool should **request the next row** from the CSV tool and continue.\nThis loop should run until all rows are exhausted.\nIf the CSV has **n rows**, then the tool must produce exactly **n `.lvdash.json` files**.\nAll generated files should be **uploaded to the GitHub output repo**.\nThe output filenames must follow a strict naming convention:\n`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026 up to `vis_n.lvdash.json`.\n\n---\n\n## **Input Requirements**\n\n* GitHub credentials and input files are in the GitHub input directory:\n  `{{GitHub_Details_For_ThoughtSpot_LVDash}}`\n* Must use the **GitHub Tools** to:\n*file name is also mentioned in the same input \n  1. Read CSV row-by-row.\n  2. Generate `.lvdash.json` for each row.\n  3. Upload each file to GitHub with name format: `vis_<row_number>.lvdash.json`.\n\n---\n\n## **Output**\n\n* Multiple `.lvdash.json` files (one per visualization row).\n* Valid JSON, Lakeview schema compliant.\n* Complete mapping of ThoughtSpot widgets \u2192 Lakeview visuals.\n* Uploaded directly into GitHub repo via GitHub Tools.\n\n---\n Example:\n\n* If the CSV has **N rows**, the output will be **N files** uploaded to GitHub.\n* Uploaded as:\n\n  * `vis_1.lvdash.json`\n  * `vis_2.lvdash.json`\n  * \u2026\n  * `vis_N.lvdash.json`\n\n---\n",
                        "expectedOutput": "## **Expected Output**\n* Display the ThoughtSpot LVDash output.\n"
                    },
                    "llm": {
                        "model": "gpt-4o",
                        "aiEngine": "AzureOpenAI",
                        "temperature": 0.10000000149011612,
                        "maxToken": 4000,
                        "topP": 1.0,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "####################################################################################",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview",
                        "bedrockModelId": null,
                        "region": null,
                        "accessKey": null,
                        "secretKey": null,
                        "vertexAIEndpoint": null,
                        "gcpProjectId": null,
                        "gcpLocation": null
                    },
                    "embedding": null,
                    "tools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true,
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nimport re\nfrom typing import Type, Any\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_writer.log'\n)\nlogger = logging.getLogger('GitHubFileWriterTool')\n\n\nclass GitHubFileWriterSchema(BaseModel):\n    folder_name: str = Field(..., description=\"Name of the folder to create in GitHub repo\")\n    file_name: str = Field(..., description=\"Name of the file to create in the folder\")\n    content: str = Field(..., description=\"Content to write to the GitHub file\")\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token\")\n\n\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates folders and files in GitHub repository\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component):\n        sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content):\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo, branch, token, folder_name, file_name, content):\n        sanitized_folder = folder_name\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check if file exists\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch})\n            sha = response.json().get(\"sha\") if response.status_code == 200 else None\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {str(e)}\", exc_info=True)\n            sha = None\n\n        payload = {\n            \"message\": f\"Add or update file: {sanitized_file}\",\n            \"content\": encoded_content,\n            \"branch\": branch\n        }\n        if sha:\n            payload[\"sha\"] = sha\n\n        try:\n            put_response = requests.put(url, json=payload, headers=headers)\n            put_response.raise_for_status()\n            return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file to GitHub: {str(e)}\", exc_info=True)\n            return f\"\u274c Failed to upload file '{sanitized_file}' to GitHub. Error: {str(e)}\"\n\n    def _run(self, folder_name: str, file_name: str, content: str, repo: str, branch: str, token: str) -> Any:\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n"
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n"
                        },
                        {
                            "toolId": 546,
                            "toolName": "DI_Github_CSV_Reader",
                            "toolClassName": "GitHubCSVRowReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nimport pandas as pd\nfrom io import StringIO\nfrom typing import Type, Any, Optional\n\n# Setup logging for the CSV row reader tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_csv_reader.log'\n)\nlogger = logging.getLogger('GitHubCSVRowReaderTool')\n\nclass GitHubCSVRowReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubCSVRowReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_path: str = Field(..., description=\"Path to the CSV file in the repository (e.g., 'data/input.csv')\")\n    branch: str = Field(..., description=\"Branch name to read the file from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n    row_number: Optional[int] = Field(None, description=\"Row number to fetch (1-based). Leave empty to return total row count.\")\n\nclass GitHubCSVRowReaderTool(BaseTool):\n    name: str = \"GitHub CSV Row Reader Tool\"\n    description: str = \"Reads a CSV file from GitHub. Returns total row count if no row_number is provided, otherwise returns the specific row.\"\n    args_schema: Type[BaseModel] = GitHubCSVRowReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_csv_from_github(self, repo: str, file_path: str, branch: str, token: str) -> pd.DataFrame:\n        \"\"\"Fetches CSV content from GitHub and loads it into a DataFrame.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching CSV file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            df = pd.read_csv(StringIO(decoded_content))\n            logger.info(f\"\u2705 Successfully loaded CSV '{file_path}' with {len(df)} rows.\")\n            return df\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch CSV: {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_path: str, branch: str, token: str, row_number: Optional[int] = None) -> Any:\n        \"\"\"Main execution logic.\"\"\"\n        df = self.fetch_csv_from_github(repo, file_path, branch, token)\n\n        if row_number is None:\n            return {\"total_rows\": len(df)}\n\n        if row_number < 1 or row_number > len(df):\n            return {\"error\": f\"\u274c Row number {row_number} is out of range. CSV has {len(df)} rows.\"}\n\n        row_data = df.iloc[row_number - 1].to_dict()\n        return {\"row_number\": row_number, \"data\": row_data}\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    csv_file = \"data/example.csv\"\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubCSVRowReaderTool()\n\n        # Example 1: Get total rows\n        result = reader_tool.run(\n            repo=github_repo,\n            file_path=csv_file,\n            branch=github_branch,\n            token=github_token\n        )\n        print(\"Total rows:\", result)\n\n        # Example 2: Get first row\n        result = reader_tool.run(\n            repo=github_repo,\n            file_path=csv_file,\n            branch=github_branch,\n            token=github_token,\n            row_number=1\n        )\n        print(\"First row:\", result)\n\n        # Example 3: Get second row\n        result = reader_tool.run(\n            repo=github_repo,\n            file_path=csv_file,\n            branch=github_branch,\n            token=github_token,\n            row_number=2\n        )\n        print(\"Second row:\", result)\n"
                        }
                    ]
                }
            }
        ],
        "langfuse": {
            "langfuseHost": "https://ava-metrics-internal.avateam.io/",
            "langfusePublicKey": "##########################################",
            "langfuseSecretKey": "##########################################"
        },
        "tasksOutputs": [
            {
                "description": "** ThoughtSpot to Lakeview LVdash Generation**\n\n**Instruction for creating the LVdash file**:\n\n---\n\n\\*\\* ThoughtSpot to Lakeview LVdash Generation\\*\\*\n\n**Instruction for creating the LVdash file**:\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to GitHub.\n\n---\n\nThe Lvdash.json must include:\n\n1. **Datasets**:\n\n   * Each dataset must have a `name`, `displayName`, and `queryLines`.\n   * The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\n\n2. **Pages**:\n\n   * Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\n   * The layout must include widgets, each with:\n\n     * `name`: unique widget name.\n     * `queries`: list of queries using the dataset and selecting specific fields.\n     * `spec`: visualization details like type, encoding, frame, and version.\n\n3. **UI Settings**:\n\n   * Theme settings for alignment and appearance.\n\nMake sure:\n*must use Github file writer tool to update file in the Github repo with the file\n*must use the measures and query from the csv and generate the proper sql query according to that every time\n* Field names and expressions match the dataset\u2019s query output columns.\n* Queries reference the correct dataset by `datasetName`.\n* Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\n* The JSON structure follows the example provided and is syntactically correct.\n\n\n### **Important Instruction \u2013 Handling Global Filter File Only**\n\n* **If the user provides only the global filters file as input**, and **no CSV file is provided**, the prompt should:\n\n  1. Generate **only one `.lvdash.json` file**, representing the global filters definition.\n  2. The output must follow the **Databricks global filter JSON structure** used in the GitHub repo.\n  3. The file must be uploaded using the **GitHub file writer tool** to the appropriate global filter folder.\n  4. All other steps in the prompt (reading CSV, generating per-visualization files, etc.) must be skipped.\n  5. Use **generic/dummy table and column names** (e.g. `ex_schema.ex_table`) in the output JSON, not actual names from the input file.\n  6. Ensure that the structure is valid and follows the required schema, with fields like `name`, `displayName`, `layout`, `widget`, `queries`, `spec`, `position`, and `pageType` properly defined.\n\n---\n\n###  **Sample Output for Global Filter JSON**\nStrictly Follow this Structure if the user give only the filter file no need to follow other sample output\nUse the following as a guide when creating the file. Do not copy actual data; use dummy names that match the structure:\nMust follow only these structure when the user mention only the filter file to the input \n```json\n{\n  \"name\": \"global_filter_001\",\n  \"displayName\": \"Global Filters\",\n  \"layout\": [\n    {\n      \"widget\": {\n        \"name\": \"widget_abc123\",\n        \"queries\": [\n          {\n            \"name\": \"main_query\",\n            \"query\": {\n              \"datasetName\": \"example_dataset\",\n              \"fields\": [\n                {\n                  \"name\": \"example_date\",\n                  \"expression\": \"`example_date`\"\n                },\n                {\n                  \"name\": \"example_count\",\n                  \"expression\": \"COUNT_IF(`filter_condition`)\"\n                }\n              ],\n              \"disaggregated\": false\n            }\n          }\n        ],\n        \"spec\": {\n          \"version\": 2,\n          \"widgetType\": \"filter-date-picker\",\n          \"encodings\": {\n            \"fields\": [\n              {\n                \"fieldName\": \"example_date\",\n                \"displayName\": \"Example Date\",\n                \"queryName\": \"dashboards/example_dashboard_id/datasets/example_dataset_example_date\"\n              }\n            ]\n          },\n          \"frame\": {\n            \"showTitle\": true\n          },\n          \"selection\": {\n            \"defaultSelection\": {\n              \"values\": {\n                \"dataType\": \"DATETIME\",\n                \"values\": [\n                  {\n                    \"value\": \"2025-01-01T00:00:00.000\"\n                  }\n                ]\n              }\n            }\n          }\n        }\n      },\n      \"position\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"width\": 1,\n        \"height\": 2\n      }\n    }\n  ],\n  \"pageType\": \"PAGE_TYPE_GLOBAL_FILTERS\",\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\n```\n\n---\n\nThis sample shows how the global filter definition should be structured, with placeholder names like `example_dataset`, `example_date`, and `example_count`. Your agent must use similar structures when generating the output file.\n\n---\n---\n\n## **Input Files**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n* **Global Filters File and page name (.txt)** \u2192 contains global filter definitions applicable across visuals. The file specifies column filters with metadata such as date filters, operator types, and possible values. These filters must be linked with relevant visuals based on the columns they use in the dataset.\n*for finding the appropriate page name use the ID(example: Vis_1, and so on) to match the input and output with this ID\n*For the global filters whenever the mentioned column or table name is used you should use that filter connect using this logic\n\n### **Important Additional Instructions for Filters**\n\n* The agent must read the global filters text file and **match filters to visuals by column name**.\n* If a column referenced in the filter is used in the dataset of a visualization, the filter must be applied to that visualization\u2019s page under the `filters` section.\n* Filters must include details such as `fieldName`, `displayName`, `type`, `isMandatory`, and `isSingleValue` where applicable.\n* The `page` object in the output JSON must correctly reference the `name` of the page that the visualization belongs to.\n* The connection between visuals and filters must be established by matching columns used in the dataset\u2019s queries to those listed in the global filters file.\n* The final JSON must include **sample and generic table names and column names** when giving example output, not the exact names from the input files.\n\n> **Do not touch other parts of the prompt or its structure.** Only add this filter handling logic to the process and alignment rules.\n\n---\n\n## **Process**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n* **Global Filters File (.txt)** \u2192 contains filters for columns used in visuals.\n\nRead the **CSV row-by-row** from GitHub:\n\n* Each row corresponds to **one visualization**.\n* For each row, generate a **separate .lvdash.json file**.\n* Continue until **all rows in the CSV are processed**.\n* For each visualization, match relevant global filters from the filters file to the dataset\u2019s columns and add them under the `filters` section of the corresponding `page` in the JSON.\n* Connect the page name referenced in the layout to the correct `page.name`.\n\nMap ThoughtSpot visuals \u2192 Databricks Lakeview widget equivalents.\n\nAssemble `.lvdash.json`:\n\n* Reference datasets = SQL views created by SQL Agent.\n* Ensure all field names in encodings exactly match SQL aliases.\n* Apply filters from the global filters file based on column usage in queries.\n* Structure follows Lakeview schema (metadata, widgets, positions, queries).\n\nSave and upload each file to GitHub:\n\n* File name convention: **`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026** matching the row index from the CSV.\n\nThe global filters must be added to the `filters` array of the correct page in each JSON file based on the dataset\u2019s usage.\n\n---\n\n## **Alignment Rules**\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\n* Never use raw ThoughtSpot expressions.\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\n* Filters from the global filters file must be connected to the relevant page if their column is used in the dataset.\n\n---\n\n## **Filter Handling Rules**\n\n* Read global filters from the provided text file.\n* For each visualization, examine the dataset\u2019s fields to see which columns are used.\n* If any column is referenced in the filters file, map the filter to that page\u2019s `filters` array in the JSON.\n* Include mandatory details: `fieldName`, `displayName`, `type`, `isMandatory`, `isSingleValue`.\n* Match columns by name irrespective of case or table alias differences where contextually appropriate.\n* Ensure filters are correctly associated with visuals by resolving the page\u2019s layout and name mapping.\n\n---\n \nWhen the input chart type is stacked column, set spec.widgetType to bar; when the input chart type is column, set spec.widgetType to table.\nFor Sankey visuals, always structure the spec.encodings as:\n\nUse \"widgetType\": \"sankey\"\n\"encodings\": {\n  \"value\": { \"fieldName\": \"<measure>\", \"displayName\": \"<measure label>\" },\n  \"stages\": [\n    { \"fieldName\": \"stage1\", \"displayName\": \"stage1\" },\n    { \"fieldName\": \"stage2\", \"displayName\": \"stage2\" },\n    ...\n  ]\n}\nDo not use \"columns\" for Sankey. Always set \"version\": 1 for Sankey visuals (not 3).\n\n\n## **Example of Filter Mapping in JSON**\n\n```json\n\"filters\": [\n  {\n    \"fieldName\": \"date\",\n    \"displayName\": \"Date\",\n    \"type\": \"FILTER\",\n    \"isMandatory\": false,\n    \"isSingleValue\": false\n  },\n  {\n    \"fieldName\": \"region\",\n    \"displayName\": \"Region\",\n    \"type\": \"FILTER\",\n    \"isMandatory\": false,\n    \"isSingleValue\": true\n  }\n]\n```\n\n---\n\n## **Rules to use the GitHub CSV Tool**\n\n* At the first run, the tool should use the GitHub CSV tool to **read the total number of rows** in the CSV.\n* It should capture this row count and use it to control the iteration.\n* Then the tool must **process the CSV row by row**, where each row corresponds to exactly **one visualization**.\n* For each row, the row\u2019s input should be passed to the LVdash generation logic to create a `.lvdash.json` file.\n* Once a row is processed, the tool should **request the next row** from the CSV tool and continue.\n* This loop should run until all rows are exhausted.\n* If the CSV has **n rows**, then the tool must produce exactly **n `.lvdash.json` files**.\n* All generated files should be **uploaded to the GitHub output repo**.\n* The output filenames must follow a strict naming convention:\n  `vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026 up to `vis_n.lvdash.json`.\n\n---\n\n## **Input Requirements**\n\n* GitHub credentials and input files are in the GitHub input directory:\n  `{\"CSV File name\": \"websitesample.csv\", \"Input repo name\": \"Input\", \"SQL file name\": \"ThoughtSpot_SQL_1.sql\", \"row_number\": 13, \"output repo\": \"Output\"}`\n* Must use the **GitHub Tools** to:\n\n  1. Read CSV row-by-row.\n  2. Read the global filters file and associate applicable filters with each visualization\u2019s dataset.\n  3. Generate `.lvdash.json` for each row.\n  4. Upload each file to GitHub with name format: `vis_<row_number>.lvdash.json`.\n\n---\n\n## **Output**\n\n* Multiple `.lvdash.json` files (one per visualization row).\n* Valid JSON, Lakeview schema compliant.\n* Complete mapping of ThoughtSpot widgets \u2192 Lakeview visuals.\n* Filters from the global filters file correctly mapped and included.\n* Uploaded directly into GitHub repo via GitHub Tools.\n\n---\n\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to GitHub.\n\n---\nThe Lvdash.json must include:\n\n1. **Datasets**:\n   - Each dataset must have a `name`, `displayName`, and `queryLines`.\n   - The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\n\n2. **Pages**:\n   - Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\n   - The layout must include widgets, each with:\n     - `name`: unique widget name.\n     - `queries`: list of queries using the dataset and selecting specific fields.\n     - `spec`: visualization details like type, encoding, frame, and version.\n\n3. **UI Settings**:\n   - Theme settings for alignment and appearance.\n\nMake sure:\n- Field names and expressions match the dataset\u2019s query output columns.\n- Queries reference the correct dataset by `datasetName`.\n- Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\n- The JSON structure follows the example provided and is syntactically correct.\n\nBelow is an example structure you must follow. Replace dataset names, queries, and fields according to the input provided.\n\n## **Input Files**\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\n\n---\n\n\n\n## **Process**\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\n* **CSV File from GitHub** \u2192 each row represents one visualization.\nRead the **CSV row-by-row** from GitHub:\n\n   * Each row corresponds to **one visualization**.\n   * For each row, generate a **separate .lvdash.json file**.\n   * Continue until **all rows in the CSV are processed**.\n Map ThoughtSpot visuals \u2192 Databricks Lakeview widget equivalents.\n Assemble `.lvdash.json`:\n\n   * Reference datasets = SQL views created by SQL Agent.\n   * Ensure all field names in encodings exactly match SQL aliases.\n   * Structure follows Lakeview schema (metadata, widgets, positions, queries).\nSave and upload each file to GitHub:\n\n   * File name convention: **`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026** matching the row index from the CSV.\nJSON structure for defining datasets and dashboards in a visualization tool. The JSON must meet the following requirements:\n\nThe dataset\u2019s queryLines must be a single string containing the complete SQL query, not split across multiple lines.\n\nThe field names defined in the widget\u2019s fields array must exactly match the columns returned by the dataset\u2019s query.\n\nNaming conventions should be consistent, using template-friendly names like page_1 or main_query.\n\nThe widget\u2019s frame title should clearly describe the data being visualized.\n\nThe structure must follow proper JSON formatting with no syntax errors or ambiguous references.\n\nProvide a sample correct JSON structure for a dataset and a line chart widget that shows daily new devices and cumulative devices over time, using a clean SQL query and consistent naming.\n\nmust follow these rules while creating generating stacked column charts, set spec.widgetType to bar (Databricks uses bar for both regular and stacked column charts).\n\nWhen converting KPI visuals from ThoughtSpot, always generate them as counter widgets in the LVdash JSON (set spec.widgetType = counter instead of table).\n\nWhen generating a pie chart, always pre-aggregate the measure in the SQL query using GROUP BY <dimension> (e.g., SUM(measure)), and in the spec.encodings use the aggregated alias (not raw column) for angle while mapping the category field to color.\n\nWhen generating LVdash JSON for a visual, always check the flattened CSV input for its corresponding tab (page) name. Use that tab name as the pages.displayName\n---\n\n## **Alignment Rules**\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\n* Never use raw ThoughtSpot expressions.\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\n\n---\n\u201cIn every queries block, always set the name field to main_query.\u201d\nThat will force the agent to output:\n\"queries\": [\n  {\n    \"name\": \"main_query\",\n    \"query\": {\n      ...\n    }\n  }\n]\n## **Line Chart Rules**\n\n* Aggregate measures explicitly (`SUM(field)` \u2192 alias `sum(Field)`).\n* Always use `DATE_TRUNC(\"MONTH\", \u2026)` for time dimensions with `\"scale\": {\"type\": \"temporal\"}`.\n* Never treat time dimensions as categorical in line charts.\n\n---\nSAMPLE outputs:\n{\n  \"datasets\": [\n    {\n      \"name\": \"example_dataset\",\n      \"displayName\": \"Example Dataset\",\n      \"queryLines\": [\n        \"SELECT DATE_TRUNC('DAY', activity_time) as daily_activity, SUM(active_users) as total_users, SUM(SUM(active_users)) OVER (ORDER BY DATE_TRUNC('DAY', activity_time) ROWS UNBOUNDED PRECEDING) as cumulative_users FROM example_schema.example_activity_view GROUP BY DATE_TRUNC('DAY', activity_time) ORDER BY daily_activity\"\n      ]\n    }\n  ],\n  \"pages\": [\n    {\n      \"name\": \"overview_page\",\n      \"displayName\": \"User Activity Overview\",\n      \"layout\": [\n        {\n          \"widget\": {\n            \"name\": \"user_activity_chart\",\n            \"queries\": [\n              {\n                \"name\": \"activity_query\",\n                \"query\": {\n                  \"datasetName\": \"example_dataset\",\n                  \"fields\": [\n                    { \"name\": \"daily_activity\", \"expression\": \"`daily_activity`\" },\n                    { \"name\": \"total_users\", \"expression\": \"`total_users`\" },\n                    { \"name\": \"cumulative_users\", \"expression\": \"`cumulative_users`\" }\n                  ],\n                  \"disaggregated\": false\n                }\n              }\n            ],\n            \"spec\": {\n              \"version\": 3,\n              \"widgetType\": \"line\",\n              \"encodings\": {\n                \"x\": {\n                  \"fieldName\": \"daily_activity\",\n                  \"displayName\": \"Daily Activity\",\n                  \"scale\": { \"type\": \"temporal\" }\n                },\n                \"y\": {\n                  \"fields\": [\n                    {\n                      \"fieldName\": \"total_users\",\n                      \"displayName\": \"Total Users\"\n                    },\n                    {\n                      \"fieldName\": \"cumulative_users\",\n                      \"displayName\": \"Cumulative Users\"\n                    }\n                  ],\n                  \"scale\": { \"type\": \"quantitative\" }\n                }\n              },\n              \"frame\": {\n                \"title\": \"Daily and Cumulative User Activity\",\n                \"showTitle\": true\n              }\n            }\n          },\n          \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\n        }\n      ],\n      \"pageType\": \"PAGE_TYPE_CANVAS\",\n      \"filters\": []\n    }\n  ],\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\nSample output for Single visuals:\n{\n  \"datasets\": [\n    {\n      \"name\": \"your_dataset_name\",\n      \"displayName\": \"Your Dataset Display Name\",\n      \"queryLines\": [\n        \"SELECT column1 AS field1, column2 AS field2, SUM(column3) OVER (ORDER BY column1 ROWS UNBOUNDED PRECEDING) AS cumulative_field FROM your_database.your_table GROUP BY column1, column2 ORDER BY column1\"\n      ]\n    }\n  ],\n  \"pages\": [\n    {\n      \"name\": \"page_1\",\n      \"displayName\": \"Your Page Title\",\n      \"layout\": [\n        {\n          \"widget\": {\n            \"name\": \"line_chart_widget\",\n            \"queries\": [\n              {\n                \"name\": \"main_query\",\n                \"query\": {\n                  \"datasetName\": \"your_dataset_name\",\n                  \"fields\": [\n                    { \"name\": \"field1\", \"expression\": \"`field1`\" },\n                    { \"name\": \"field2\", \"expression\": \"`field2`\" },\n                    { \"name\": \"cumulative_field\", \"expression\": \"`cumulative_field`\" }\n                  ],\n                  \"disaggregated\": false\n                }\n              }\n            ],\n            \"spec\": {\n              \"version\": 3,\n              \"widgetType\": \"line\",\n              \"encodings\": {\n                \"x\": {\n                  \"fieldName\": \"field1\",\n                  \"displayName\": \"Field 1 Label\",\n                  \"scale\": { \"type\": \"temporal\" }\n                },\n                \"y\": {\n                  \"fields\": [\n                    {\n                      \"fieldName\": \"field2\",\n                      \"displayName\": \"Field 2 Label\"\n                    },\n                    {\n                      \"fieldName\": \"cumulative_field\",\n                      \"displayName\": \"Cumulative Field Label\"\n                    }\n                  ],\n                  \"scale\": { \"type\": \"quantitative\" }\n                }\n              },\n              \"frame\": {\n                \"title\": \"Your Chart Title\",\n                \"showTitle\": true\n              }\n            }\n          },\n          \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\n        }\n      ],\n      \"pageType\": \"PAGE_TYPE\",\n      \"filters\": []\n    }\n  ],\n  \"uiSettings\": {\n    \"theme\": {\n      \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\n    }\n  }\n}\n\n## **Encoding Rules**\n\n* **Single field axis** \u2192\n\n```json\n\"y\": {\n  \"fieldName\": \"Sales\",\n  \"displayName\": \"Sales\",\n  \"scale\": { \"type\": \"quantitative\" }\n}\n```\n\n* **Multi-measure axis** \u2192\n\n```json\n\"y\": {\n  \"fields\": [\n    { \"fieldName\": \"Processed\", \"displayName\": \"Processed\" },\n    { \"fieldName\": \"Shipped\", \"displayName\": \"Shipped\" }\n  ],\n  \"scale\": { \"type\": \"quantitative\" }\n}\n```\n\n---\n\n## **Dataset Mapping Rules**\n\n* Every dataset in `.lvdash.json` must directly map to a SQL SELECT statement from `metric_views.sql`.\n* Column names must exactly match aliases.\n* Never alias an aggregated field with the same name as its raw column.\n\n---\nRules to use the github csv tool:\n---\n\nAt the first run, the tool should use the GitHub CSV tool to **read the total number of rows** in the CSV.\nIt should capture this row count and use it to control the iteration.\nThen the tool must **process the CSV row by row**, where each row corresponds to exactly **one visualization**.\nFor each row, the row\u2019s input should be passed to the LVdash generation logic to create a `.lvdash.json` file.\nOnce a row is processed, the tool should **request the next row** from the CSV tool and continue.\nThis loop should run until all rows are exhausted.\nIf the CSV has **n rows**, then the tool must produce exactly **n `.lvdash.json` files**.\nAll generated files should be **uploaded to the GitHub output repo**.\nThe output filenames must follow a strict naming convention:\n`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026 up to `vis_n.lvdash.json`.\n\n---\n\n## **Input Requirements**\n\n* GitHub credentials and input files are in the GitHub input directory:\n  `{\"CSV File name\": \"websitesample.csv\", \"Input repo name\": \"Input\", \"SQL file name\": \"ThoughtSpot_SQL_1.sql\", \"row_number\": 13, \"output repo\": \"Output\"}`\n* Must use the **GitHub Tools** to:\n*file name is also mentioned in the same input \n  1. Read CSV row-by-row.\n  2. Generate `.lvdash.json` for each row.\n  3. Upload each file to GitHub with name format: `vis_<row_number>.lvdash.json`.\n\n---\n\n## **Output**\n\n* Multiple `.lvdash.json` files (one per visualization row).\n* Valid JSON, Lakeview schema compliant.\n* Complete mapping of ThoughtSpot widgets \u2192 Lakeview visuals.\n* Uploaded directly into GitHub repo via GitHub Tools.\n\n---\n Example:\n\n* If the CSV has **N rows**, the output will be **N files** uploaded to GitHub.\n* Uploaded as:\n\n  * `vis_1.lvdash.json`\n  * `vis_2.lvdash.json`\n  * \u2026\n  * `vis_N.lvdash.json`\n\n---\n\n\nYou have access to the the tool 'GitHubFileWriterTool'. It is advised to use this tool when you have access to it irrespective of whether it has been asked to be used explicitly.\n\nYou have access to the the tool 'GitHubFileReaderTool'. It is advised to use this tool when you have access to it irrespective of whether it has been asked to be used explicitly.\n\nYou have access to the the tool 'GitHubCSVRowReaderTool'. It is advised to use this tool when you have access to it irrespective of whether it has been asked to be used explicitly.",
                "expected_output": "## **Expected Output**\n* Display the ThoughtSpot LVDash output.\n",
                "summary": "** ThoughtSpot to Lakeview LVdash Generation**\n\n**Instruction for creating the LVdash...",
                "raw": "The provided GitHub token is either invalid or lacks the necessary permissions to access the repository. Please provide a valid token with the required access rights to proceed."
            }
        ],
        "output": "The provided GitHub token is either invalid or lacks the necessary permissions to access the repository. Please provide a valid token with the required access rights to proceed.",
        "enableAgenticMemory": false
    }
}